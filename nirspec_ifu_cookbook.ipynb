{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce178b9",
   "metadata": {},
   "source": [
    "# NIRSpec IFU Data Reduction Pipeline\n",
    "\n",
    "By the JWST ERS TEMPLATES team (B. Welch, T. Hutchison, +). \n",
    "Pre-release version, December 2023. \n",
    "\n",
    "Based on a combination of notebooks from STSCI:  \n",
    "https://github.com/STScI-MIRI/MRS-ExampleNB/blob/main/Flight_Notebook1/MRS_FlightNB1.ipynb\n",
    "https://github.com/spacetelescope/jwebbinar_prep/blob/main/spec_mode/spec_mode_stage_2.ipynb\n",
    "https://github.com/spacetelescope/jwebbinar_prep/blob/main/spec_mode/spec_mode_stage_3.ipynb\n",
    "https://github.com/spacetelescope/jwebbinar_prep/blob/main/ifu_session/jwebbinar5_nirspecifu.ipynb\n",
    "\n",
    "Prerequisites: Install JWST pipeline. See TEMPLATES pipeline installation notebook (https://github.com/JWST-Templates/Notebooks/blob/main/0_install_pipeline.ipynb) for help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb0ad5",
   "metadata": {},
   "source": [
    "# Example Pipeline Run on NIRSpec IFU Data\n",
    "\n",
    "The output printed by the various pipeline stages is extensive -- which is great for troubleshooting and learning the pipeline but also makes these notebooks very large in size (which GitHub hates).  We've added an `stpipe-log.cfg` file to this repository that will pipe all of that printed output into a log file called `pipeline-run.log`, which will be generated in the same working directory.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select target name for easier processing below\n",
    "# MAKE SURE TARGET NAME MATCHES DATAFILE HEADERS!\n",
    "\n",
    "target = 'SGAS1723'\n",
    "#target = 'SGAS1226'\n",
    "#target = 'SPT0418-47'\n",
    "#target = 'SPT2147-50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d09de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Modify the paths to the relevant directories on your machine\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1) point to where the jwst pipeline config files are located\n",
    "home = \"/Users/me/home/\" # update with your home directory\n",
    "\n",
    "\n",
    "# 2) point to where you keep your uncalibrated data (*uncal.fits)\n",
    "if target == 'SGAS1723':\n",
    "    input_path = os.path.join(home,'path/to/target/data/')\n",
    "if target == 'SGAS1226':\n",
    "    input_path = os.path.join(home,'path/to/target/data/')\n",
    "if target == 'SPT0418-47':\n",
    "    input_path = os.path.join(home,'path/to/target/data/')\n",
    "if target == 'SPT2147-50':\n",
    "    input_path = os.path.join(home,'path/to/target/data/')\n",
    "\n",
    "\n",
    "# 3) point to where you want your processed outputs to live\n",
    "\n",
    "# the pipeline will automatically use the most recent CRDS pmap reference file\n",
    "pmap = 'pmap1105' # so, replace this value with that most recent version number\n",
    "\n",
    "if target == 'SGAS1723':\n",
    "    output_path = os.path.join(home,f'path/to/target/output/{pmap}/')\n",
    "if target == 'SGAS1226':\n",
    "    output_path = os.path.join(home,f'path/to/target/output/{pmap}/')\n",
    "if target == 'SPT0418-47':\n",
    "    output_path = os.path.join(home,f'path/to/target/output/{pmap}/')\n",
    "if target == 'SPT2147-50':\n",
    "    output_path = os.path.join(home,f'path/to/target/output/{pmap}/')\n",
    "\n",
    "if os.path.exists(output_path) == False: # if folder doesn't exist\n",
    "    print('Creating folder ' + output_path)\n",
    "    os.system('mkdir ' + output_path) # creates the folder\n",
    "\n",
    "\n",
    "###############################################\n",
    "# Set up CRDS path and server environment variables\n",
    "os.environ[\"CRDS_PATH\"] = home + \"crds_cache/jwst_ops\"\n",
    "os.environ[\"CRDS_SERVER_URL\"] = \"https://jwst-crds.stsci.edu\"\n",
    "###############################################\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "import json\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "import astropy.units as u\n",
    "from astropy import wcs\n",
    "from astropy.wcs import WCS\n",
    "from astropy.visualization import ImageNormalize, ManualInterval, LogStretch, LinearStretch, AsinhStretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calwebb_spec and spec3 pipelines\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# the level1 pipeline:\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "\n",
    "# data models\n",
    "from jwst import datamodels\n",
    "\n",
    "# association file utilities\n",
    "from jwst.associations import asn_from_list as afl # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base # Definition of a Lvl3 association file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thou shalt know thy software version!\n",
    "# We recommend updating your pipeline software regularly (pip install --upgrade jwst)\n",
    "import jwst\n",
    "print(jwst.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3b416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stage 1 - Detector-level processing\n",
    "The first time the pipeline runs it will be glacially slow, as many GB of reference files need to be downloaded. With the CRDS environment variables set above, updated pmap reference files will be downloaded automatically each time the pipeline is run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cb000-c5dd-44d5-a9d1-1a98304e9970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: this a test to make sure that the files are accessible (useful troubleshooting)\n",
    "# note that your program may name your TARGPROP differently\n",
    "\n",
    "files = glob.glob(input_path + '*nrs*/*_uncal.fits') #list the uncalibrated (level 1b) files.\n",
    "files = sorted(files)\n",
    "\n",
    "ifu,sky = 0,0\n",
    "\n",
    "for exposure in files:\n",
    "    test = fits.open(exposure)\n",
    "    head = test[0].header\n",
    "    # NOTE the target name format can change for each observing program\n",
    "    if head['TARGPROP'] == target+'-IFU': ifu+=1 \n",
    "    elif head['TARGPROP'] == target+'-SKY': sky+=1\n",
    "    elif head['TARGPROP'] == target+'-IFU-OFFSET': sky+=1\n",
    "    else: print('not target, ', head[\"TARGPROP\"])\n",
    "    \n",
    "print(f'Number of IFU: {ifu}, Number of sky: {sky}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb23da6-bbd8-4f81-9645-11155e1f3de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking that the file system is in place for these data\n",
    "# if not, creating the folders\n",
    "folders_L2a = ['L2a/','L2a/sci/','L2a/bkg/']\n",
    "\n",
    "for folder in folders_L2a:\n",
    "    if os.path.exists(output_path + folder) == False: # if folder doesn't exist\n",
    "        print('Creating folder ' + output_path + folder)\n",
    "        os.system('mkdir ' + output_path + folder) # creates the folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df98a7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "To control parameters directly from the notebook, the below version works. Simply add \"det1.[step].[param] = [val]\" before calling \"det1(exposure)\"\n",
    "For example, to utilize multiple cores, add the line \"det1.jump.maximum_cores = 'half'\" and \"det1.ramp_fit.maximum_cores = 'half'\" to use half the available cores in those two steps.\n",
    "\n",
    "Changing the maximum_cores variable in the parameter file can speed up this step. The default is \"None\", which uses one core. Other options are \"quarter\", \"half\", and \"all\", which will use 1/4, 1/2, or all available cores respectively. We find that \"half\" provides good performance while still allowing your computer to perform other simple tasks.\n",
    "\n",
    "In previous pipeline versions, the \"expand_large_events\" flag for the jump detection step was turned off by default. It is now recommended that this flag be set to True to better remove cosmic ray snowballs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline, splitting outputs into \"sci\" and \"bkg\" output folders\n",
    "# If \"leak calibration\" exposures were taken, they will be in the same folders \n",
    "# as their associated observations\n",
    "\n",
    "files = glob.glob(input_path + '*nrs*/*_uncal.fits') #list the uncalibrated (level 1b) files.\n",
    "files = sorted(files)\n",
    "\n",
    "for exposure in files: \n",
    "    det1 = Detector1Pipeline()\n",
    "    # set output directory based on sci vs bkg exposure\n",
    "    head = fits.open(exposure)[0].header\n",
    "    if head['TARGPROP'] == target+'-IFU':\n",
    "        det1.output_dir = output_path + 'L2a/sci'\n",
    "    elif (head['TARGPROP'] == target+'-SKY') or (head['TARGPROP'] == target+'-IFU-OFFSET'):\n",
    "        det1.output_dir = output_path + 'L2a/bkg'\n",
    "    else: print('not target')\n",
    "    # now we set other parameters:\n",
    "    det1.save_results = True\n",
    "    det1.jump.maximum_cores = 'half'\n",
    "    det1.jump.expand_large_events = True\n",
    "    det1.ramp_fit.maximum_cores = 'half'\n",
    "\n",
    "    det1(exposure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74e5d7",
   "metadata": {},
   "source": [
    "### Correct 1/f noise\n",
    "The NIRSpec detectors are read-noise limited. To correct this read noise, we utilize the NSClean package (B. Rauscher, https://ui.adsabs.harvard.edu/abs/2023arXiv230603250R/abstract). Basically this fits the detector noise in Fourier space using unilluminated pixels set by a user-defined mask. We build the mask using pre-existing cal.fits files (can be downloaded from MAST).  \n",
    "\n",
    "Here we only correct the science exposures, as that is all we plan to use going forward. Our current understanding is that the leakcals and background exposures only serve to add noise (see TEMPLATES overview paper for more details: ***LINK***). At least for TEMPLATES, the lack of bright stars on the closed MSA means the leakcals aren't really providing any improvement. And the background exposures almost perfectly match the predicted backgrounds. So we ignore the leakcals, and subtract the expected background after the pipeline has been run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make \"destriped\" L2a files\n",
    "folders_L2a_destripe = ['L2a_destripe','L2a_destripe/sci/','L2a_destripe/bkg/']\n",
    "\n",
    "for folder in folders_L2a_destripe:\n",
    "    if os.path.exists(output_path + folder) == False: # if folder doesn't exist\n",
    "        print('Creating folder ' + output_path + folder)\n",
    "        os.system('mkdir ' + output_path + folder) # creates the folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d50c4",
   "metadata": {},
   "source": [
    "NSClean requires that all science pixels be properly masked, so that only the detector noise is included in the fit. To do this, we utilize pre-existing stage 2 pipeline products (cal.fits files), which mask out everything but the science pixels. These can be downloaded from MAST if needed. We have experimented with masking pixels flagged as snowballs, but we have found that this leads to artifacts in the output files from NSClean (likely because not enough pixels are available in certain columns to get a good fit). We thus have commented out this part of the mask making code. Large snowballs can be masked manually if they are found to be problematic. The central rows containing the fixed slits are also masked.\n",
    "\n",
    "This code was largely written by Ian Wong, modified to work with NSClean by B. Welch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make masks using existing cal.fits files:\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "def make_mask(ratefiles, calfiles, output_dir=None):\n",
    "    sortrate = sorted(ratefiles)\n",
    "    sortcal = sorted(calfiles)\n",
    "    masks = []\n",
    "    for i, fi in enumerate(sortrate):\n",
    "        hdu = fits.open(fi)\n",
    "        dq = hdu['DQ',1].data\n",
    "        spec_dq = dq == 4 # == 4 masks all jump detections. Use == 0 to only use pixels with no data quality flags\n",
    "        cal = sortcal[i]\n",
    "        # make initial mask from cal file\n",
    "        spec_mask = ~np.isnan(fits.getdata(cal))\n",
    "        # create a bit of a buffer\n",
    "        spec_mask = nd.binary_dilation(spec_mask, iterations=2)\n",
    "        # Also mask out fixed slit region at center of detectors\n",
    "        spec_mask[900:1100,:] = True\n",
    "        #full_mask = np.logical_or(spec_mask,spec_dq) # commented out if not using DQ flags in automated masking step\n",
    "        full_mask = ~spec_mask #full_mask\n",
    "        \n",
    "        if output_dir:\n",
    "            hdu = fits.PrimaryHDU(full_mask.astype(float))\n",
    "            hdu.writeto(os.path.join(output_dir, fi.split(\"/\")[-1].replace('rate','mask2')), overwrite=True)\n",
    "        else:\n",
    "            output_dir = os.path.dirname(fi)\n",
    "            hdu = fits.PrimaryHDU(full_mask.astype(float))\n",
    "            hdu.writeto(os.path.join(output_dir, fi.split(\"/\")[-1].replace('rate','mask2')), overwrite=True)\n",
    "            output_dir = None\n",
    "\n",
    "        masks.append(full_mask)\n",
    "    return masks\n",
    "\n",
    "\n",
    "# select rate/cal files to use - probably in a different directory\n",
    "if target == 'SGAS1723':\n",
    "    ratefiles = sorted(glob.glob(output_path + folders_L2a[2] + 'jw01355028001_0[2,6]101*rate.fits'))\n",
    "    caldir = 'path/to/existing/cal/files/' # update with your directory\n",
    "if target == 'SGAS1226':\n",
    "    ratefiles = sorted(glob.glob(output_path + folders_L2a[0] + 'sci/jw*_02101*rate.fits', recursive=True))\n",
    "    caldir = 'path/to/existing/cal/files/' # update with your directory\n",
    "if target == 'SPT0418-47':\n",
    "    ratefiles = sorted(glob.glob(output_path + folders_L2a[0] + 'sci/jw*_02101*nrs1_rate.fits', recursive=True))\n",
    "    caldir = 'path/to/existing/cal/files/' # update with your directory\n",
    "if target == 'SPT2147-50':\n",
    "    ratefiles = sorted(glob.glob(output_path + folders_L2a[0] + 'sci/jw*_02101*nrs1_rate.fits', recursive=True))\n",
    "    caldir = 'path/to/existing/cal/files/' # update with your directory\n",
    "\n",
    "calfiles = sorted(glob.glob(caldir + '**/*cal.fits', recursive=True))\n",
    "maskdir = os.path.join(output_path, 'destripe_masks/')\n",
    "if os.path.exists(maskdir) == False: # if folder doesn't exist\n",
    "    os.system('mkdir ' + maskdir) # creates the folder\n",
    "\n",
    "masks = make_mask(ratefiles, calfiles, output_dir=None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03dfbac-59ac-4a45-8cd1-d44edb7a54e2",
   "metadata": {},
   "source": [
    "*Setting up NSClean & importing the package*:  \n",
    "\n",
    "We recommend downloading the NSClean code [from the JWST \"Publications for Scientists\" website](https://webb.nasa.gov/content/forScientists/publications.html), placing it somewhere that you can point to, & unzipping the file.  We'll manually import the package in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ddd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSClean Setup:\n",
    "# Configuration needs to come first\n",
    "os.environ['MKL_NUM_THREADS'] = '8'   # Enable multi-threading\n",
    "os.environ['NSCLEAN_USE_CUPY'] = 'NO' # Use GPU. If you change this, you will\n",
    "                                         #   need to restart python. In practice,\n",
    "                                         #   I have found that most of the time\n",
    "                                         #   the GPU is no faster than using CPUs.\n",
    "\n",
    "# Import the appropriate numerical libraries\n",
    "if os.getenv('NSCLEAN_USE_CUPY') == 'YES':\n",
    "    import cupy as cp\n",
    "    import numpy as np\n",
    "else:\n",
    "    import numpy as cp\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "# importing NSClean package\n",
    "import sys \n",
    "sys.path.append('/Users/me/path/to/NSClean/') # to import the nsclean module faster\n",
    "from nsclean import nsclean as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# NRS1 FILE DESTRIPING\n",
    "##################\n",
    "\n",
    "if target == 'SGAS1723':\n",
    "    # NOTE this only checks the science directory L2a/sci\n",
    "    files = sorted(glob.glob(output_path + folders_L2a[1] + 'jw*_0[2,6]*_nrs1_rate.fits', recursive=True))\n",
    "else:\n",
    "    files = sorted(glob.glob(output_path + folders_L2a[1] + 'jw*_02101*_nrs1_rate.fits', recursive=True))\n",
    "\n",
    "maskfiles = sorted(glob.glob(output_path+ folders_L2a[0] + '**/*nrs1_mask2.fits', recursive=True))\n",
    "\n",
    "\n",
    "for i in np.arange(len(files)):\n",
    "    print(f'starting file {i}')\n",
    "    \n",
    "    # Download FITS file\n",
    "    hdul = fits.open(files[i])\n",
    "    H0 = hdul[0].header\n",
    "    H1 = hdul[1].header\n",
    "    D1 = cp.array(hdul[1].data)\n",
    "    #hdul.close()\n",
    "    \n",
    "    D1[np.isnan(D1)] = 0.0 # cleaner doesn't like nans, so set them to zero\n",
    "    \n",
    "    # Instantiate an NSClean object\n",
    "    M = fits.getdata(maskfiles[i]).astype(bool)\n",
    "    cleaner = nc.NSClean('NRS1', M)\n",
    "\n",
    "    # Clean it\n",
    "    D1 = cleaner.clean(D1, buff=True)\n",
    "    \n",
    "    # Save it to FITS\n",
    "    H0['comment'] = 'Processed by NSClean Rev. '+nc.__version__\n",
    "    if os.getenv('NSCLEAN_USE_CUPY') == 'YES':\n",
    "        hdul[1].data = cp.asnumpy(D1)\n",
    "    else:\n",
    "        hdul[1].data = D1\n",
    "    \n",
    "    \n",
    "    # final version\n",
    "    hdul.writeto(output_path + folders_L2a_destripe[1] + \\\n",
    "                     nc.chsuf(os.path.basename(files[i]), '.cln_mask.fits'),\n",
    "                        overwrite=True)\n",
    "    hdul.close()\n",
    "    print(f'Done file {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# NRS2 FILE DESTRIPING\n",
    "##################\n",
    "\n",
    "if target == 'SGAS1723':\n",
    "    # NOTE this only checks the science directory L2a/sci\n",
    "    files = sorted(glob.glob(output_path + folders_L2a[1] + 'jw*_0[2,6]*_nrs2_rate.fits', recursive=True))\n",
    "else:\n",
    "    files = sorted(glob.glob(output_path + folders_L2a[1] + 'jw*_02101*_nrs2_rate.fits', recursive=True))\n",
    "\n",
    "maskfiles = sorted(glob.glob(output_path+ folders_L2a[0] + '**/*nrs2_mask2.fits', recursive=True))\n",
    "\n",
    "# Instantiate an NSClean object\n",
    "cleaner = nc.NSClean('NRS2', M)\n",
    "\n",
    "for i in np.arange(len(files)):\n",
    "    print(f'starting file {i}')\n",
    "        \n",
    "    # Download FITS file\n",
    "    hdul = fits.open(files[i])\n",
    "    H0 = hdul[0].header\n",
    "    H1 = hdul[1].header\n",
    "    D1 = cp.array(hdul[1].data)\n",
    "    #hdul.close()\n",
    "    \n",
    "    D1[np.isnan(D1)] = 0.0 # cleaner doesn't like nans, so set them to zero\n",
    "    \n",
    "    # Instantiate an NSClean object\n",
    "    M = fits.getdata(maskfiles[i]).astype(bool)\n",
    "    cleaner = nc.NSClean('NRS2', M)\n",
    "\n",
    "    # Clean it\n",
    "    D1 = cleaner.clean(D1, buff=True)\n",
    "    \n",
    "    # Save it to FITS\n",
    "    H0['comment'] = 'Processed by NSClean Rev. '+nc.__version__\n",
    "    if os.getenv('NSCLEAN_USE_CUPY') == 'YES':\n",
    "        hdul[1].data = cp.asnumpy(D1)\n",
    "    else:\n",
    "        hdul[1].data = D1\n",
    "\n",
    "    # final version\n",
    "    hdul.writeto(output_path + folders_L2a_destripe[1] + \\\n",
    "                     nc.chsuf(os.path.basename(files[i]), '.cln_mask.fits'),\n",
    "                        overwrite=True)\n",
    "    hdul.close()\n",
    "    print(f'Done file {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f9db5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stage 2 - Produce calibrated exposures\n",
    "These next two cells assume that you have two directories with the L2a outputs from the Stage 1 pipeline as defined above. We have named these \"sci\" for the science observations, and \"bkg\" for the background observations.  \n",
    "\n",
    "As the pipeline is set up here, no background subtraction is performed in this stage. \n",
    "\n",
    "This is the stage where leak calibration exposures can be used to remove any stray light leaking through the MSA onto the detectors. We do not currently apply that correction, as it simply introduces additional noise and we do not find significant light leakage in our observations. We have included code previously used to do this leakcal subtraction, however we will not be supporting that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47dedf-3581-4d25-a930-e5288fab2d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking that the file system is in place for these data\n",
    "# if not, creating the folders\n",
    "folders_L2b = ['L2b_destripe/','L2b_destripe/sci/','L2b_destripe/bkg/']\n",
    "\n",
    "for folder in folders_L2b:\n",
    "    if os.path.exists(output_path + folder) == False: # if folder doesn't exist\n",
    "        print('Creating folder ' + output_path + folder)\n",
    "        os.system('mkdir ' + output_path + folder) # creates the folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0c47a",
   "metadata": {
    "tags": []
   },
   "source": [
    "To include leakcals (and background subtraction if desired) in the Stage 2 pipeline, an association file is needed. Below is a function to create these asn files from a combination of science, leak, and background exposures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets adapt the writel3asn function to produce what we want for the level 2 pipeline\n",
    "def writel2asn(scifiles, leakfiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMSLevel2bBase, product_name=prodname)\n",
    "    \n",
    "    # Add leakcal files to the association\n",
    "    if leakfiles:\n",
    "        for ii in range(len(leakfiles)):\n",
    "            asn['products'][0]['members'].append({'expname': leakfiles[ii], 'exptype': 'imprint'})\n",
    "            \n",
    "    # Add background files to the association\n",
    "    if bgfiles:\n",
    "        nbg=len(bgfiles)\n",
    "        for ii in range(0,nbg):\n",
    "            asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffc0c0",
   "metadata": {},
   "source": [
    "For TEMPLATES, we have found that the leakcals (observations taken with MSA/IFU all closed) only introduce noise into our final products. TEMPLATES are all fairly dark fields (not many bright stars nearby) so leakcals only add detector noise, and do not remove any real leaking light. We thus do not process leakcals (use_leak flag set to False). If you wish to subtract leakcals, set use_leak = True. This will subtract leakcals on a dither-by-dither basis, assuming each science dither includes a corresponding leakcal. We leave a commented-out block of code at the end which creates an average leakcal to subtract from science exposures. We do not commit to maintain this version, but leave it for reference.\n",
    "\n",
    "We have also found that subtracting backgrounds using specific background pointings introduces more noise than we would prefer. We therefore subtract the background using the JWST backgrounds tool after the Stage 3 pipeline has been run. If you would prefer to use dedicated background pointings (e.g. if you are observing below 1 micron, for which the background tool has some known errors), set the process_background flag to True.\n",
    "\n",
    "Below is a block of code that creates the appropriate asn files for each exposure. This involves a lot of tedious file grouping, leading to the large and somewhat messy-looking code blocks. We have tried to include many comments to clarify the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has become a huge mess and I'm sorry\n",
    "\n",
    "\n",
    "# \"dither\" leak_version will use each leakcal with its associated sci exposure\n",
    "# \"all\" will average all leakcals and subtract the average from each sci exposure\n",
    "leak_version = 'dither' # 'dither' or 'all'\n",
    "use_leak = False # actually use leakcals?\n",
    "############\n",
    "\n",
    "\n",
    "process_background = False # actually process backgrounds??\n",
    "#############\n",
    "\n",
    "\n",
    "# define some path names for easy access\n",
    "level1_sci_dir = os.path.join(output_path, folders_L2a_destripe[1]) \n",
    "level1_bg_dir = os.path.join(output_path, folders_L2a_destripe[2]) \n",
    "#leakdir_sci = os.path.join(output_path, 'L2a/leaksci')\n",
    "#leakdir_bg = os.path.join(output_path, 'L2a/leakbkg')\n",
    "level2outputdir = os.path.join(output_path, folders_L2b[0])\n",
    "\n",
    "# define variables for file naming \n",
    "# user will have to look these up and edit them\n",
    "\n",
    "program_id = '01355' # 5-digit string. ID of observing program. '01355' for TEMPLATES\n",
    "if target == 'SGAS1723':\n",
    "    obs_num_sci = ['007','027'] # 3-digit string. Observation number for science observations. \n",
    "    # This is a List for 1723 b/c we are using failed observations where possible\n",
    "    obs_num_bkg = '028' # 3-digit string. Observation number for background observations. \n",
    "    visit_num = '001' # 3-digit string. Visit number. '001' for most observations\n",
    "    group_sci = ['02101', '06101'] # List of 5-digit strings. Visit group numbers for science observations. \n",
    "    # For S1723, '02' is g140h observations, '06' is g395h observations\n",
    "    group_leak = ['04101', '08101'] # List of 5-digit strings. Visit group numbers for leakcals. \n",
    "    # For S1723, '04' and '08' are dedicated leakcals\n",
    "    # Group numbers should be findable in the observation APT file\n",
    "    \n",
    "if target == 'SGAS1226':\n",
    "    obs_num_sci = '001' # 3-digit string. Observation number for science observations. \n",
    "    obs_num_bkg = '002' # 3-digit string. Observation number for background observations. \n",
    "    visit_num = '001' # 3-digit string. Visit number. '001' for most observations\n",
    "    group_sci = ['02101'] # List of 5-digit strings. Visit group numbers for science observations. \n",
    "    group_leak = ['02103'] # List of 5-digit strings. Visit group numbers for leakcals. \n",
    "    group_leak_bg = ['04101']\n",
    "    # for 1226, the leakcals have different group numbers for the off-target pointing. No idea why. 'Tis silly.\n",
    "\n",
    "if target == 'SPT0418-47':\n",
    "    obs_num_sci = '011'\n",
    "    obs_num_bkg = '012'\n",
    "    visit_num = '001'\n",
    "    group_sci = ['02101']\n",
    "    group_leak = ['02103']\n",
    "    \n",
    "if target == 'SPT2147-50':\n",
    "    obs_num_sci = '019'\n",
    "    obs_num_bkg = '020'\n",
    "    visit_num = '001'\n",
    "    group_sci = ['02101']\n",
    "    group_leak = ['04101']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702f274-7f5d-4403-9c04-67c4cbef7830",
   "metadata": {},
   "source": [
    "Take care when defining all of these files. Getting the wrong science, background, and leakcal exposures wrong can really mess up the final product. We print the output association files, and recommend checking the contents of these files prior to running the Spec2 pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae38408",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target == 'SGAS1723':\n",
    "    endfile = 'rate.cln_mask' \n",
    "    # and combine some of these for convenience:\n",
    "    sci_base = 'jw' + program_id + obs_num_sci[0] + visit_num + '_' # I always feel like\n",
    "    bkg_base = 'jw' + program_id + obs_num_bkg + visit_num + '_' # somebody's watching meeee\n",
    "    # second set of failed observations gets defined separately\n",
    "    sci_base2 = 'jw' + program_id + obs_num_sci[1] + visit_num + '_' \n",
    "\n",
    "    # specify science files\n",
    "    # currently assumes we use 2 gratings. Will need to edit for more/fewer grating observations\n",
    "    scifiles_g1 = sorted(glob.glob(level1_sci_dir + sci_base + group_sci[0] + '*'+endfile+'.fits'))\n",
    "    # add in second set of failed observations\n",
    "    scifiles_g1 = sorted(scifiles_g1 + glob.glob(level1_sci_dir + sci_base2 + group_sci[0] + '*'+endfile+'.fits'))\n",
    "    scifiles_g2 = sorted(glob.glob(level1_sci_dir + sci_base2 + group_sci[1] + '*'+endfile+'.fits'))\n",
    "\n",
    "    # and background files:\n",
    "    bgfiles_g1 = sorted(glob.glob(level1_bg_dir + bkg_base + group_sci[0] + '*'+endfile+'.fits'))\n",
    "    bgfiles_g2 = sorted(glob.glob(level1_bg_dir + bkg_base + group_sci[1] + '*'+endfile+'.fits'))\n",
    "\n",
    "    # collect leakcal files\n",
    "    sci_leakfiles_g1_all = sorted(glob.glob(level1_sci_dir + sci_base + group_leak[0] + '*'+endfile+'.fits'))\n",
    "    sci_leakfiles_g2_all = sorted(glob.glob(level1_sci_dir + sci_base + group_leak[1] + '*'+endfile+'.fits'))\n",
    "\n",
    "    bg_leakfiles_g1_all = sorted(glob.glob(level1_bg_dir + bkg_base + group_leak[1] + '*'+endfile+'.fits'))\n",
    "    bg_leakfiles_g2_all = sorted(glob.glob(level1_bg_dir + bkg_base + group_leak[1] + '*'+endfile+'.fits'))\n",
    "\n",
    "    filemin, filemax = -17, -10\n",
    "    if endfile != 'rate':\n",
    "        diff = len('rate') - len(endfile)\n",
    "        filemin += diff\n",
    "        filemax += diff \n",
    " \n",
    "    # this version assumes you have the same number of dithers for science, leak, and background observations\n",
    "    if leak_version == 'dither':\n",
    "        for i,file in enumerate(scifiles_g1):\n",
    "            #leak = sci_leakfiles_g1_all[i]\n",
    "            expnum = file[filemin:filemax]\n",
    "            if i < 8:\n",
    "                outfile = 'jw'+program_id+'-o'+obs_num_sci[0]+'_'+group_sci[0]+'_spec2_'+expnum+'_asn.json' \n",
    "            else:\n",
    "                outfile = 'jw'+program_id+'-o'+obs_num_sci[1]+'_'+group_sci[0]+'_spec2_'+expnum+'_asn.json'\n",
    "            outfile = os.path.join(level2outputdir, outfile)\n",
    "            if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "            else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "            print(outfile)\n",
    "\n",
    "        for i,file in enumerate(scifiles_g2): # comment out this loop if only one grating exists\n",
    "            #leak = sci_leakfiles_g2_all[i] \n",
    "            expnum = file[filemin:filemax]\n",
    "            outfile = 'jw'+program_id+'-o'+obs_num_sci[1]+'_'+group_sci[1]+'_spec2_'+expnum+'_asn.json'\n",
    "            outfile = os.path.join(level2outputdir, outfile)\n",
    "            if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "            else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "            print(outfile)\n",
    "\n",
    "        if process_background == True:\n",
    "            for i,file in enumerate(bgfiles_g1):\n",
    "                #leak = bg_leakfiles_g1_all[i]\n",
    "                expnum = file[filemin:filemax]\n",
    "                outfile = 'jw'+program_id+'-o'+obs_num_bkg+'_'+group_sci[0]+'_spec2_'+expnum+'_asn.json'\n",
    "                outfile = os.path.join(level2outputdir, outfile)\n",
    "                if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "                else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "                print(outfile)\n",
    "\n",
    "            for i,file in enumerate(bgfiles_g2): # comment out this loop if only one grating exists\n",
    "                #leak = bg_leakfiles_g2_all[i]\n",
    "                expnum = file[filemin:filemax]\n",
    "                outfile = 'jw'+program_id+'-o'+obs_num_bkg+'_'+group_sci[1]+'_spec2_'+expnum+'_asn.json'\n",
    "                outfile = os.path.join(level2outputdir, outfile)\n",
    "                if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "                else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "                print(outfile)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68052304",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target == 'SGAS1226':\n",
    "    endfile = 'rate.cln_mask'\n",
    "    # and combine some of these for convenience:\n",
    "    sci_base = 'jw' + program_id + obs_num_sci + visit_num + '_' # I always feel like\n",
    "    bkg_base = 'jw' + program_id + obs_num_bkg + visit_num + '_' # somebody's watching meeee\n",
    "\n",
    "    # specify science files\n",
    "    scifiles_g1 = sorted(glob.glob(level1_sci_dir + sci_base + group_sci[0] + '*rate.cln_mask.fits'))\n",
    "\n",
    "    # and background files:\n",
    "    bgfiles_g1 = sorted(glob.glob(level1_bg_dir + bkg_base + group_sci[0] + '*rate.cln_mask.fits'))\n",
    "\n",
    "    # collect leakcal files\n",
    "    sci_leakfiles_g1_all = sorted(glob.glob(level1_sci_dir + sci_base + group_leak[0] + '*rate.cln_mask.fits'))\n",
    "    # because SGAS1226 has different numbers, we use the group_leak_bg variable here\n",
    "    bg_leakfiles_g1_all = sorted(glob.glob(level1_bg_dir + bkg_base + group_leak_bg[0] + '*rate.cln_mask.fits'))\n",
    "\n",
    "    filemin, filemax = -17, -10\n",
    "    if endfile != 'rate':\n",
    "        diff = len('rate') - len(endfile)\n",
    "        filemin += diff\n",
    "        filemax += diff \n",
    "\n",
    "    # this version assumes you have the same number of dithers for science, leak, and background observations\n",
    "    if leak_version == 'dither':\n",
    "        for i,file in enumerate(scifiles_g1):\n",
    "            #leak = sci_leakfiles_g1_all[i]\n",
    "            expnum = file[filemin:filemax]\n",
    "            outfile = 'jw'+program_id+'-o'+obs_num_sci+'_'+group_sci[0]+'_spec2_'+expnum+'_asn.json' \n",
    "            outfile = os.path.join(level2outputdir, outfile)\n",
    "            if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "            else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "            print(outfile)\n",
    "\n",
    "        if process_background == True:\n",
    "            for i,file in enumerate(bgfiles_g1):\n",
    "                #leak = bg_leakfiles_g1_all[i]\n",
    "                expnum = file[filemin:filemax]\n",
    "                outfile = 'jw'+program_id+'-o'+obs_num_bkg+'_'+group_sci[0]+'_spec2_'+expnum+'_asn.json'\n",
    "                outfile = os.path.join(level2outputdir, outfile)\n",
    "                if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "                else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "                print(outfile)\n",
    "\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84709e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (target == 'SPT0418-47') or (target == 'SPT2147-50'):\n",
    "    endfile = 'rate.cln_mask'\n",
    "    # and combine some of these for convenience:\n",
    "    sci_base = 'jw' + program_id + obs_num_sci + visit_num + '_' # I always feel like\n",
    "    bkg_base = 'jw' + program_id + obs_num_bkg + visit_num + '_' # somebody's watching meeee\n",
    "\n",
    "    # specify science files\n",
    "    scifiles_g1 = sorted(glob.glob(level1_sci_dir + sci_base + group_sci[0] + '*nrs1_rate.cln_mask.fits'))\n",
    "\n",
    "    # and background files:\n",
    "    bgfiles_g1 = sorted(glob.glob(level1_bg_dir + bkg_base + group_sci[0] + '*nrs1_rate.cln_mask.fits'))\n",
    "\n",
    "    # collect leakcal files\n",
    "    sci_leakfiles_g1_all = sorted(glob.glob(level1_sci_dir + sci_base + group_leak[0] + '*nrs1_rate.cln_mask.fits'))\n",
    "    # because SGAS1226 has different numbers, we use the group_leak_bg variable here\n",
    "    bg_leakfiles_g1_all = sorted(glob.glob(level1_bg_dir + bkg_base + group_leak[0] + '*nrs1_rate.cln_mask.fits'))\n",
    "\n",
    "    filemin, filemax = -17, -10\n",
    "    if endfile != 'rate':\n",
    "        diff = len('rate') - len(endfile)\n",
    "        filemin += diff\n",
    "        filemax += diff \n",
    "\n",
    "    # this version assumes you have the same number of dithers for science, leak, and background observations\n",
    "    if leak_version == 'dither':\n",
    "        for i,file in enumerate(scifiles_g1):\n",
    "            #leak = sci_leakfiles_g1_all[i]\n",
    "            expnum = file[filemin:filemax]\n",
    "            outfile = 'jw'+program_id+'-o'+obs_num_sci+'_'+group_sci[0]+'_spec2_'+expnum+'_asn.json' \n",
    "            outfile = os.path.join(level2outputdir, outfile)\n",
    "            if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "            else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "            print(outfile)\n",
    "\n",
    "        if process_background == True:\n",
    "            for i,file in enumerate(bgfiles_g1):\n",
    "                #leak = bg_leakfiles_g1_all[i]\n",
    "                expnum = file[filemin:filemax]\n",
    "                outfile = 'jw'+program_id+'-o'+obs_num_bkg+'_'+group_sci[0]+'_spec2_'+expnum+'_asn.json'\n",
    "                outfile = os.path.join(level2outputdir, outfile)\n",
    "                if use_leak: writel2asn([file], [leak], None, outfile, 'Level2') \n",
    "                else: writel2asn([file], None, None, outfile, 'Level2') \n",
    "                print(outfile)\n",
    "\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50774af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below version will use average of all leakcals, and subtract that from each science exposure\n",
    "# we are not supporting this currently, but leave it in case anyone wants to attempt to resurrect it\n",
    "\n",
    "'''\n",
    "leakfiles_04_nrs1 = sorted(glob.glob(leakdir_sci + '/jw01355027001_04101_*nrs1_rate.fits'))\n",
    "leakfiles_04_nrs2 = sorted(glob.glob(leakdir_sci + '/jw01355027001_04101_*nrs2_rate.fits'))\n",
    "leakfiles_08_nrs1 = sorted(glob.glob(leakdir_sci + '/jw01355027001_08101_*nrs1_rate.fits'))\n",
    "leakfiles_08_nrs2 = sorted(glob.glob(leakdir_sci + '/jw01355027001_08101_*nrs2_rate.fits'))\n",
    "bg_leakfiles_04_nrs1 = sorted(glob.glob(leakdir_bg + '/jw01355028001_04101_*nrs1_rate.fits'))\n",
    "bg_leakfiles_04_nrs2 = sorted(glob.glob(leakdir_bg + '/jw01355028001_04101_*nrs2_rate.fits'))\n",
    "bg_leakfiles_08_nrs1 = sorted(glob.glob(leakdir_bg + '/jw01355028001_08101_*nrs1_rate.fits'))\n",
    "bg_leakfiles_08_nrs2 = sorted(glob.glob(leakdir_bg + '/jw01355028001_08101_*nrs2_rate.fits'))\n",
    "\n",
    "\n",
    "if leak_version == 'all':\n",
    "    for file in scifiles_02:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_04_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_04_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o027_02101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "\n",
    "    for file in scifiles_06:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_08_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_08_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o027_06101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "        \n",
    "    for file in bgfiles_02:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_04_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_04_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o028_02101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "\n",
    "    for file in bgfiles_06:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_08_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_08_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o028_06101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we run the pipeline with the asn files we just made! \n",
    "\n",
    "if target == 'SGAS1723':\n",
    "    asnfiles_sci = glob.glob(level2outputdir+'jw'+program_id+'-o'+obs_num_sci[0]+'*asn.json') \n",
    "    asnfiles_sci += glob.glob(level2outputdir+'jw'+program_id+'-o'+obs_num_sci[1]+'*asn.json')\n",
    "else:\n",
    "    asnfiles_sci = glob.glob(level2outputdir+'jw'+program_id+'-o'+obs_num_sci+'*asn.json')\n",
    "asnfiles_bkg = glob.glob(level2outputdir+'jw'+program_id+'-o'+obs_num_bkg+'*asn.json')\n",
    "\n",
    "for asn in asnfiles_sci:\n",
    "    spec2 = Spec2Pipeline()\n",
    "    spec2.output_dir = output_path + folders_L2b[1]\n",
    "    spec2.save_results = True\n",
    "    spec2(asn)\n",
    "\n",
    "if process_background == True:\n",
    "    for asn in asnfiles_bkg:\n",
    "        spec2 = Spec2Pipeline()\n",
    "        spec2.output_dir = output_path + folders_L2b[2]\n",
    "        spec2.save_results = True\n",
    "        spec2(asn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1754446",
   "metadata": {},
   "source": [
    "## Stage 3 - Creating final data cubes\n",
    "This stage compiles the individual calibrated exposures into a single, (theoretically) science-ready data cube. We first have to define an association file, which tells the pipeline which files to use as science exposures, and which files to use as backgrounds.\n",
    "\n",
    "As mentioned above, for TEMPLATES we are subtracting a smooth, calculated background after Stage 3 processing. If instead you wish to subtract an observed background, set the process_background flag above to True to include background pointings in the Stage 3 association file. \n",
    "\n",
    "#### NOTE on outlier detection\n",
    "When we started creating this notebook, outlier_detection was dysfunctional. T. Hutchison therefore developed our own sigma clipping routine (in a separate notebook). The pipeline outlier_detection step has now been fixed (versions 1.11.3 and later), however it may require some parameter tuning to give the best results. We have found that the best performance comes from using the updated pipeline outlier detection step alongside our custom routine (see Hutchison et al. for details: ***LINK***). We also include a first-pass cut on the Stage 2 files to remove the most egregious outliers, but most of the final cleaning comes in the outlier detection step in the pipeline (and then the post-processing algorithm from Hutchison et al.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01dc63-9e20-4803-b49f-257b7e5ac7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that the file system is in place for these data\n",
    "# if not, creating the folders\n",
    "folders_L3 = ['L3_destripe/']\n",
    "\n",
    "for folder in folders_L3:\n",
    "    if os.path.exists(output_path + folder) == False: # if folder doesn't exist\n",
    "        print('Creating folder ' + output_path + folder)\n",
    "        os.system('mkdir ' + output_path + folder) # creates the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW COPIED FROM DAVID LAW'S MIRI MRS NOTEBOOK: \n",
    "# https://github.com/STScI-MIRI/MRS-ExampleNB/blob/main/Flight_Notebook1/MRS_FlightNB1.ipynb\n",
    "# \n",
    "# Define a useful function to write out a Lvl3 association file from an input list\n",
    "# Note that any background exposures have to be of type x1d.\n",
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    # Add background files to the association\n",
    "    if bgfiles:\n",
    "        nbg=len(bgfiles)\n",
    "        for ii in range(0,nbg):\n",
    "            asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d83853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COURTESY OF D. LAW:\n",
    "# Manually remove most egregious outliers from the cal.fits files\n",
    "\n",
    "from stcal import dqflags\n",
    "\n",
    "\n",
    "def cut_cal(infile, outfile, max_threshold=20, min_threshold=-1):\n",
    "    hdu=fits.open(infile)\n",
    "    sci=hdu['SCI'].data\n",
    "    dq=hdu['DQ'].data\n",
    "    \n",
    "    dnubit=dqflags.interpret_bit_flags('DO_NOT_USE', mnemonic_map=datamodels.dqflags.pixel)\n",
    "    indx=np.where((dq & dnubit) != 0)\n",
    "    sci[indx]=np.nan\n",
    "\n",
    "    indx=np.where((sci > max_threshold) | (sci < min_threshold))\n",
    "    sci[indx]=np.nan\n",
    "    dq[indx] = np.bitwise_or(dq[indx], dnubit)\n",
    "    \n",
    "    hdu['SCI'].data=sci\n",
    "    hdu.writeto(outfile, overwrite=True)\n",
    "    \n",
    "\n",
    "orig_calfiles = glob.glob(level2outputdir + '/sci/*cal.fits') # cal files output from L2 pipeline\n",
    "\n",
    "# set different thresholds for each target - done by-eye w/ L2 ratefiles\n",
    "if target == 'SGAS1723': \n",
    "    calmax_blue = 2000\n",
    "    calmax_red = 350\n",
    "    calmax = 2000\n",
    "#if target == 'SGAS1723': calmax = 350 # second cut for redder grating\n",
    "if target == 'SGAS1226': calmax = 50\n",
    "if target == 'SPT0418-47': calmax = 40\n",
    "if target == 'SPT2147-50': calmax = 30\n",
    "\n",
    "calfiles = glob.glob(level2outputdir + '/sci/*cal.fits') # cal files output from L2 pipeline\n",
    "\n",
    "for file in calfiles:\n",
    "    outfile = file[:-5] + '2.fits'\n",
    "    cut_cal(file, outfile, max_threshold=calmax, min_threshold=-10)\n",
    "    print(outfile)\n",
    "\n",
    "bkg_calfiles = glob.glob(level2outputdir + '/bkg/*cal.fits') # cal files output from L2 pipeline\n",
    "if process_background == True:\n",
    "    for file in bkg_calfiles:\n",
    "        outfile = file[:-5] + '2.fits'\n",
    "        cut_cal(file, outfile, max_threshold=calmax, min_threshold=-10)\n",
    "        print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make association files \n",
    "calfiles = glob.glob(output_path + folders_L2b[1] + '*cal2.fits')\n",
    "\n",
    "bkgfiles_cal = glob.glob(output_path + folders_L2b[2] + '*cal2.fits') \n",
    "\n",
    "# We have included several versions to generate Spec3 association files\n",
    "# 'nobg' runs only the science observations, without subtracting any background (This is what we recommend)\n",
    "# 'bgonly' was set up to only process the background pointings, primarily as an internal test\n",
    "# 'standard' implements the pipeline's background subtraction, which we do NOT recommend using\n",
    "version = 'nobg' \n",
    "\n",
    "if version == 'standard':\n",
    "    if len(folders_L3) == 1:\n",
    "        fold = folders_L3[0]\n",
    "        asnfile = os.path.join(output_path, fold, 'L3asn.json')\n",
    "    else:\n",
    "        fold = folders_L3[1]\n",
    "        asnfile = os.path.join(output_path, fold, 'L3asn.json')\n",
    "    lev3asnname = 'Level3_' + target + '_BGSUB'\n",
    "    writel3asn(calfiles, bkgfiles, asnfile, lev3asnname) \n",
    "    \n",
    "if version == 'bgonly':\n",
    "    fold = folders_L3[0]\n",
    "    asnfile = os.path.join(output_path, fold, 'L3asn.json')\n",
    "    lev3asnname = 'Level3_' + target + '_BGONLY'\n",
    "    writel3asn(bkgfiles_cal, None, asnfile, lev3asnname)\n",
    "    \n",
    "if version == 'nobg':\n",
    "    if len(folders_L3) == 1:\n",
    "        fold = folders_L3[0]\n",
    "        asnfile = os.path.join(output_path, fold, 'L3asn.json')\n",
    "    else:\n",
    "        fold = folders_L3[3]\n",
    "        asnfile = os.path.join(output_path, fold, 'L3asn.json')\n",
    "    lev3asnname = 'Level3_' + target + '_NOBG'\n",
    "    writel3asn(calfiles, None, asnfile, lev3asnname)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e968d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Next step, call stage 3 pipeline on new asn file\n",
    "\n",
    "# setting it up this way (rather than with \"call\") gives a bit more flexibility to edit parameters on the fly\n",
    "spec3 = Spec3Pipeline()\n",
    "spec3.output_dir = output_path + fold # use folder defined in cell above - output to same dir as asn file\n",
    "#spec3.logcfg = spec3.output_dir + 'test-log.cfg'\n",
    "\n",
    "# To skip the outlier detection step, uncomment this line\n",
    "# We recommend using both the outlier detection step AND the custom \n",
    "# sigma clipping algorithm described in Hutchison et al.\n",
    "#spec3.outlier_detection.skip = True \n",
    "\n",
    "# cube building parameters:\n",
    "#spec3.cube_build.coord_system = 'ifualign' # Stay in IFU coordinates, don't rotate to N=up E=left coordinates\n",
    "# For some programs, it may be desireable to drizzle to finer pixel scales. The below commands can do that.\n",
    "#spec3.cube_build.scale1 = 0.075 # Final pixel scale, in arcsec\n",
    "#spec3.cube_build.scale2 = 0.075\n",
    "\n",
    "spec3.save_results = True # DON'T FORGET THIS OR YOU'LL WASTE SEVERAL HOURS FOR NAUGHT!\n",
    "\n",
    "spec3.run(asnfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca639c",
   "metadata": {},
   "source": [
    "## Last Step: *DIY background subtraction!*  \n",
    "\n",
    "The background pointings generally just introduce additional noise, so we are instead subtracting the expected backgrounds calculated from the JWST background tool (https://jwst-docs.stsci.edu/jwst-other-tools/jwst-backgrounds-tool). This will make the final result a bit cleaner, and the measured backgrounds match the expected backgrounds well for each TEMPLATES target (based on testing we have done). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def exp_bg_sub(cubefile, bgfile, outfile=None):\n",
    "    # first, load the expected background data:\n",
    "    exp = np.loadtxt(bgfile)\n",
    "    expwl = np.array([line[0] for line in exp])\n",
    "    exptot = np.array([line[1] for line in exp])\n",
    "    # and make an interp1d object sdo we can get the background at any given point:\n",
    "    bginterp = interp1d(expwl, exptot)\n",
    "    # now load the data cube:\n",
    "    hdu = fits.open(cubefile)\n",
    "    data = hdu[1].data\n",
    "    head1 = hdu[1].header\n",
    "    cubewl = np.arange(head1['CRVAL3'],\n",
    "                       head1['CRVAL3']+(head1['CDELT3']*len(data)),\n",
    "                       head1['CDELT3'])\n",
    "    if len(cubewl) > len(data):\n",
    "        cubewl = cubewl[:-1]\n",
    "    # next, evaluate background on data wavelength points:\n",
    "    bgcalculated = bginterp(cubewl)\n",
    "    bgcube = np.tile(bgcalculated, (data.shape[2],data.shape[1],1)).T # cubeify!\n",
    "    # and subtract!\n",
    "    #print(len(data), len(cubewl))\n",
    "    bgsub_cube = data - bgcube\n",
    "    # save or return result:\n",
    "    if outfile:\n",
    "        hdu[1].data = bgsub_cube\n",
    "        hdu[1].header['comment'] = 'Expected Background Subtracted'\n",
    "        hdu.writeto(outfile, overwrite=True)\n",
    "        hdu.close()\n",
    "    else:\n",
    "        hdu.close()\n",
    "        return bgsub_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08ca44-4db6-47e2-b28a-31c8858ac5b3",
   "metadata": {},
   "source": [
    "Run the `jwst_backgrounds` tool before running this final cell, so that the background files exist.  We recommend using a naming convention that makes sense for you -- for us, we have named the files \"[target name]_background.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target == 'SGAS1723':\n",
    "    expbg_file = f'path/to/jwst/expected_bg/{target}_background.txt'\n",
    "    g395cube = os.path.join(output_path, folders_L3[0], lev3asnname+'_g395h-f290lp_s3d.fits')\n",
    "    g140cube = os.path.join(output_path, folders_L3[0], lev3asnname+'_g140h-f100lp_s3d.fits')\n",
    "    g395out = g395cube.replace('NOBG','BGSUB')\n",
    "    g140out = g140cube.replace('NOBG','BGSUB')\n",
    "    exp_bg_sub(g140cube, expbg_file, g140out)\n",
    "    exp_bg_sub(g395cube, expbg_file, g395out)\n",
    "    \n",
    "if target == 'SGAS1226':\n",
    "    expbg_file = f'path/to/jwst/expected_bg/{target}_background.txt'\n",
    "    g235cube = os.path.join(output_path, folders_L3[0], lev3asnname+'_g235h-f170lp_s3d.fits')\n",
    "    g235out = g235cube.replace('NOBG', 'BGSUB')\n",
    "    exp_bg_sub(g235cube, expbg_file, g235out)\n",
    "    \n",
    "    \n",
    "if target == 'SPT0418-47':\n",
    "    expbg_file = f'path/to/jwst/expected_bg/{target}_background.txt'\n",
    "    g395cube = os.path.join(output_path, folders_L3[0], lev3asnname+'_g395m-f290lp_s3d.fits')\n",
    "    g395out = g395cube.replace('NOBG', 'BGSUB')\n",
    "    exp_bg_sub(g395cube, expbg_file, g395out)\n",
    "    \n",
    "\n",
    "if target == 'SPT2147-50':\n",
    "    expbg_file = f'path/to/jwst/expected_bg/{target}_background.txt'\n",
    "    g395cube = os.path.join(output_path, folders_L3[0], lev3asnname+'_g395m-f290lp_s3d.fits')\n",
    "    g395out = g395cube.replace('NOBG', 'BGSUB')\n",
    "    exp_bg_sub(g395cube, expbg_file, g395out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0acd8-f8ff-45c8-af25-82000a8b0c1f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f79da-28fe-473f-afbd-d3a455698aea",
   "metadata": {},
   "source": [
    "### allllllll done\n",
    "\n",
    "pat yourself on the back, good job team!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
