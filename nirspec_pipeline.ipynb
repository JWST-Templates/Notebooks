{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce178b9",
   "metadata": {},
   "source": [
    "# NIRSpec IFU Data Reduction Pipeline\n",
    "\n",
    "By the JWST ERS TEMPLATES team (B. Welch, T. Hutchison, +).   \n",
    "Version: Nov. 2022. \n",
    "\n",
    "Based on a combination of notebooks from STSCI:  \n",
    "- [MRS_FlightNB1](https://github.com/STScI-MIRI/MRS-ExampleNB/blob/main/Flight_Notebook1/MRS_FlightNB1.ipynb)\n",
    "- [spec_mode_stage_2](https://github.com/spacetelescope/jwebbinar_prep/blob/main/spec_mode/spec_mode_stage_2.ipynb)\n",
    "- [spec_mode_stage_3](https://github.com/spacetelescope/jwebbinar_prep/blob/main/spec_mode/spec_mode_stage_3.ipynb)\n",
    "- [jwebbinar5_nirspecifu](https://github.com/spacetelescope/jwebbinar_prep/blob/main/ifu_session/jwebbinar5_nirspecifu.ipynb)\n",
    "\n",
    "Prerequisites: Install JWST pipeline. See TEMPLATES pipeline installation notebook ([linked here](https://github.com/JWST-Templates/Notebooks/blob/main/0_install_pipeline.ipynb)) for help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb0ad5",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "# Example Pipeline Run on NIRSpec IFU Data\n",
    "Currently this is the re-observed IFU data on SGAS-1723 from the TEMPLATES data. We have set up this notebook such that you can specify which of the TEMPLATES targets you would like to use in the pipeline. \n",
    "\n",
    "### Important Note:\n",
    ">This version is applying background subtraction in the Stage 3 pipeline. It takes the Stage 2 output extracted 1-D spectra (\"x1d.fits\") from the background observations, and creates an average spectrum to universally subtract from the science data. \n",
    ">\n",
    ">There is another possibility for background subtraction, where the dedicated background exposures would be subtracted from the science exposures during the Stage 2 pipeline. These would be \"image from image\" subtractions, rather than creating a universal background spectrum. We ran into an issue with this type of background subtraction. The grating wheel positions were slightly different between the science and background observations. The stage 2 pipeline did not like that one bit, and refused to apply any background subtraction (the grating wheel position affects the wavelength solution, and thus would throw off this style of background subtraction). We may test the 2D background subtraction on another dataset, but the assumption is it will be noisier. \n",
    ">\n",
    ">Also, the outlier_detection step in the Stage 3 pipeline is currently dysfunctional (as of pipeline version 1.8.3). We include a band-aid solution for now. Hopefully this issue is fixed in later pipeline versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e613190-d61a-499e-b9b3-a2e08e69980d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Additional Note:\n",
    ">The output printed by the various pipeline stages is extensive -- which is great for troubleshooting and learning the pipeline but also makes these notebooks very large in size (which GitHub hates).  We've added an `stpipe-log.cfg` file to this repository that will pipe all of that printed output into a log file called `pipeline-run.log`, which will be generated in the same working directory.  To help those running the pipeline from scratch in tracking down what happens in each stage, we've separated that log informtion into the three different pipeline stages, which can be found in: `pipeline-stage1.log`, `pipeline-stage2.log`, `pipeline-stage3.log` (when you run this pipeline yourself, all of this log info would appear in the `pipeline-run.log` file instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0485ef52-e8b3-44e7-8ba5-8ea153d9e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------\n",
    "# PATHS: \n",
    "# -----\n",
    "\n",
    "# 1) point to where you keep your data\n",
    "# input_path = '/Users/bdwelch1/Documents/data/templates/sdss1723/full_uncal_data/' # for B. Welch\n",
    "input_path = \"/Users/tahutch1/data/raw/jwst/ers/templates/MAST_2022-10-18T1132/JWST/\" # for T. Hutchison\n",
    "\n",
    "# 2) point to where you want your processed outputs to live\n",
    "# output_path = '/Users/bdwelch1/Documents/data/templates/sdss1723/pmap1009_ditherleak_1Dbg/' # for B. Welch\n",
    "output_path = \"/Users/tahutch1/data/raw/jwst/ers/templates/reduced/\" # for T. Hutchison\n",
    "\n",
    "# 3) Un-comment these 2 lines if you haven't specified these CRDS paths elsewhere\n",
    "# -- (like described in the installation script linked at the top of this notebook)\n",
    "# os.environ[\"CRDS_PATH\"] = home + \"crds_cache/jwst_ops\"\n",
    "# os.environ[\"CRDS_SERVER_URL\"] = \"https://jwst-crds.stsci.edu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d09de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packgaes needed\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# astronomy code\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "import astropy.units as u\n",
    "from astropy import wcs\n",
    "from astropy.wcs import WCS\n",
    "from astropy.visualization import ImageNormalize, ManualInterval, LogStretch, LinearStretch, AsinhStretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a190058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54df3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Stage 1 pipeline:\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "\n",
    "# The Stage 2 & 3 pipelines:\n",
    "# -- calwebb_spec and spec3\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# data models\n",
    "from jwst import datamodels\n",
    "\n",
    "# association file utilities\n",
    "from jwst.associations import asn_from_list as afl # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base # Definition of a Lvl3 association file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb055e83-7373-4157-97d7-47a6dcb5c192",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting up variables\n",
    "First we'll look at all of the targets who we have data for, then we can specify the target want we want to reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492602a7-e5c1-4be0-8a62-c9a5dcc7444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick one of these target names and assign it to the \"target\" variable below:\n",
      "\n",
      " ['SPT2147', 'SPT0418', 'SGAS1723']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(input_path + '*nrs*/*_uncal.fits') # list the uncalibrated (level 1b) files.\n",
    "files = sorted(files)\n",
    "\n",
    "target_list = []\n",
    "\n",
    "# running through files, checking header for target name\n",
    "for exposure in files:\n",
    "    head = fits.getheader(exposure)\n",
    "    \n",
    "    # splitting the object name from data type (IFU vs SKY)\n",
    "    splitting_name = head['TARGPROP'].split('-')\n",
    "    target_list.append(splitting_name[0]) # just taking the object name\n",
    "\n",
    "targets_list = list(set(target_list))\n",
    "\n",
    "print('Pick one of these target names and assign it to the \"target\" variable below:\\n\\n', targets_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d72f345-5a21-42de-8e39-a359e62d365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'SGAS1723' # specify your target name here, as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b349f23-88a1-4727-9871-e5a9bfc7bea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder /Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/L2a/\n",
      "Creating folder /Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/L2a/sci/\n",
      "Creating folder /Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/L2a/bkg/\n",
      "Creating folder /Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/L2b/\n",
      "Creating folder /Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/L2b/sci/\n",
      "Creating folder /Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/L2b/bkg/\n",
      "Creating folder /Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/L3/\n"
     ]
    }
   ],
   "source": [
    "# checking that the file system is in place for our reduced data\n",
    "# if not, creating the folders\n",
    "\n",
    "targ_folder = target + '/'\n",
    "\n",
    "folders = ['',                           # the main folder\n",
    "           'L2a/','L2a/sci/','L2a/bkg/', # output from Stage 1\n",
    "           'L2b/','L2b/sci/','L2b/bkg/', # output from Stage 2\n",
    "           'L3/' ]                       # output from Stage 3\n",
    "\n",
    "for folder in folders:\n",
    "    path = output_path + targ_folder + folder\n",
    "    if os.path.exists(path) == False: # if folder doesn't exist\n",
    "        print('Creating folder ' + path)\n",
    "        os.system('mkdir ' + path) # creates the folder\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4d816-479c-42d6-8fd3-7b0e6c0450fe",
   "metadata": {},
   "source": [
    "#### Assigning additional variables that we'll need in Stage 2 based upon the specified TEMPLATES target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf76bfc-c162-4925-92e0-f43dc43c7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_id = '01355' # 5-digit string. ID of observing program. '01355' for TEMPLATES\n",
    "\n",
    "\n",
    "# below is a dictionary listing all of the observation+target-specific variables:\n",
    "# values chosen based on target specified above.\n",
    "#\n",
    "# YOU DO NOT NEED TO EDIT THIS DICTIONARY FOR TEMPLATES SOURCES\n",
    "obs_info = {'SGAS1723':{'obs_num_sci':'027',\n",
    "                        'obs_num_bkg':'028',\n",
    "                        'visit_num':'001',\n",
    "                        'group_sci':['02','06'], # for SGAS1723: '02' is g140h data, '06' is g395h data\n",
    "                        'group_leak':['04','08']} # associated dedicated leakcals\n",
    "           # \n",
    "           # will be adding the values for the other 3 targets soon!\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6b436c-af12-4fc1-9164-63eb206c38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the right dictionary of values based on chosen TEMPLATES target\n",
    "target_obs_info = obs_info[target]\n",
    "\n",
    "obs_num_sci = target_obs_info['obs_num_sci'] # 3-digit string. Observation number for science observations.\n",
    "obs_num_bkg = target_obs_info['obs_num_bkg'] # 3-digit string. Observation number for background observations.\n",
    "\n",
    "visit_num = target_obs_info['visit_num'] # 3-digit string. Visit number. '001' for most observations\n",
    "group_sci = target_obs_info['group_sci'] # List of 2-digit strings. Visit group numbers for science observations.\n",
    "group_leak = target_obs_info['group_leak'] # List of 2-digit strings. Visit group numbers for leakcals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4789c-96bd-42c6-b8ab-62766f922f0d",
   "metadata": {},
   "source": [
    "#### Updating output path to include target name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67795e13-0144-44ce-b266-329cbf831f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tahutch1/data/raw/jwst/ers/templates/reduced/\n",
      "\n",
      "Updated path, to include chosen target: \n",
      "/Users/tahutch1/data/raw/jwst/ers/templates/reduced/SGAS1723/\n"
     ]
    }
   ],
   "source": [
    "# printing original output path\n",
    "print(output_path,end='\\n\\n')\n",
    "\n",
    "# adding target name to the end of the output path\n",
    "output_path += targ_folder\n",
    "\n",
    "print(f'Updated path, to include chosen target: \\n{output_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3b416",
   "metadata": {
    "tags": []
   },
   "source": [
    "--------------------\n",
    "\n",
    "## Stage 1 - Detector-level processing\n",
    "The first time this runs it will be *glacially* slow, as many GB of reference files need to be downloaded.\n",
    "Changing the maximum_cores variable in the parameter file can speed up this step. The default is \"None\", which uses one core. Other options are \"quarter\", \"half\", and \"all\", which will use 1/4, 1/2, or all available cores respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df98a7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "To control parameters directly in the notebook, simply add `det1.[step].[param] = [val]` before calling `det1(exposure)`.\n",
    "For example, to utilize multiple cores, add the line `det1.jump.maximum_cores = 'half'` and `det1.ramp_fit.maximum_cores = 'half'` to use half the available cores in those two steps.\n",
    "\n",
    "The default values work well for the stage 1 pipeline. The only parameter that should be changed is the maximum_cores. \"half\" typically gives good performance without totally incapacitating your machine (assuming it has half physical and half virtual cores). \"full\" speeds things up a bit more in exchange for eating all of your computers processing power (only use this if you don't need your computer for anything else for several hours). The default option \"None\" uses one core, and is glacially slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d565d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the pipeline, splitting outputs into \"sci\" and \"bkg\" output folders\n",
    "# Leakcals will be in the same folders as their associated observations\n",
    "\n",
    "for exposure in files: \n",
    "    det1 = Detector1Pipeline()\n",
    "    \n",
    "    # set output directory based on sci vs bkg exposure\n",
    "    head = fits.open(exposure)[0].header\n",
    "    if head['TARGPROP'] == f'{target}-IFU':\n",
    "        det1.output_dir = output_path + 'L2a/sci'\n",
    "    elif head['TARGPROP'] == f'{target}-SKY':\n",
    "        det1.output_dir = output_path + 'L2a/bkg'\n",
    "    else: print('not target')\n",
    "    \n",
    "    # now we set other parameters:\n",
    "    det1.save_results = True\n",
    "    det1.jump.maximum_cores = 'half'\n",
    "    det1.ramp_fit.maximum_cores = 'half'\n",
    "\n",
    "    # run stage 1\n",
    "    det1(exposure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f9db5",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------------\n",
    "\n",
    "## Stage 2 - Produce calibrated exposures\n",
    "This stage uses the L2a outputs from the Stage 1 pipeline. We have named these \"sci\" for the science observations, and \"bkg\" for the background observations.  \n",
    "\n",
    "#### Note:\n",
    ">We have chosen to implement leak corrections (from dedicated leakcal exposures) for *both* the science and background observations. It doesn't seem to make too big of a difference (based on few short checks), but our sense is that it avoids double-subtracting light leaking through the MSA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0c47a",
   "metadata": {
    "tags": []
   },
   "source": [
    "To include leakcals (and background subtraction if desired) in the Stage 2 pipeline, an association file is needed. Below is a function to create these asn files from a combination of science, leak, and background exposures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets adapt the writel3asn function to produce what we want for the level 2 pipeline\n",
    "def writel2asn(scifiles, leakfiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMSLevel2bBase, product_name=prodname)\n",
    "    \n",
    "    # Add leakcal files to the association\n",
    "    if leakfiles:\n",
    "        for ii in range(len(leakfiles)):\n",
    "            asn['products'][0]['members'].append({'expname': leakfiles[ii], 'exptype': 'imprint'})\n",
    "            \n",
    "    # Add background files to the association\n",
    "    if bgfiles:\n",
    "        nbg=len(bgfiles)\n",
    "        for ii in range(0,nbg):\n",
    "            asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffc0c0",
   "metadata": {},
   "source": [
    "In this example, we implement the leakcal correction on a dither-by-dither basis, since each of our science and background dithers include corresponding leakcals. This seems to improve performance slightly (though for now this is likely confounded by changing reference files). Another option would be to use an average of all leakcal observations for this correction. \n",
    "\n",
    "Below is a block of code that creates the appropriate asn files for each exposure. This involves a lot of tedious file grouping, leading to the large and somewhat messy-looking code block. \n",
    "\n",
    "*We have included many comments to try to clarify the code as much as possible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae38408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"dither\" leak_version will use each leakcal with its associated sci exposure\n",
    "# \"all\" will average all leakcals and subtract the average from each sci exposure\n",
    "leak_version = 'dither' # 'dither' or 'all'\n",
    "\n",
    "\n",
    "# define some path names for easy access\n",
    "level1_sci_dir = os.path.join(output_path, 'L2a/sci/')\n",
    "level1_bg_dir = os.path.join(output_path, 'L2a/bkg/')\n",
    "level2outputdir = os.path.join(output_path, 'L2b/')\n",
    "\n",
    "\n",
    "# and combine some of these for convenience:\n",
    "sci_base = 'jw' + program_id + obs_num_sci + visit_num + '_' # I always feel like\n",
    "bkg_base = 'jw' + program_id + obs_num_bkg + visit_num + '_' # somebody's watching meeee\n",
    "\n",
    "\n",
    "# specify science files\n",
    "# currently assumes we use 2 gratings. Will need to edit for more/fewer grating observations\n",
    "scifiles_g1 = sorted(glob.glob(level1_sci_dir + sci_base + group_sci[0] + '*rate.fits'))\n",
    "scifiles_g2 = sorted(glob.glob(level1_sci_dir + sci_base + group_sci[1] + '*rate.fits'))\n",
    "\n",
    "# and background files:\n",
    "bgfiles_g1 = sorted(glob.glob(level1_bg_dir + bkg_base + group_sci[0] + '*rate.fits'))\n",
    "bgfiles_g2 = sorted(glob.glob(level1_bg_dir + bkg_base + group_sci[1] + '*rate.fits'))\n",
    "\n",
    "\n",
    "# collect leakcal files\n",
    "sci_leakfiles_g1_all = sorted(glob.glob(level1_sci_dir + sci_base + group_leak[0] + '*rate.fits'))\n",
    "sci_leakfiles_g2_all = sorted(glob.glob(level1_sci_dir + sci_base + group_leak[1] + '*rate.fits'))\n",
    "\n",
    "bg_leakfiles_g1_all = sorted(glob.glob(level1_bg_dir + bkg_base + group_leak[0] + '*rate.fits'))\n",
    "bg_leakfiles_g2_all = sorted(glob.glob(level1_bg_dir + bkg_base + group_leak[1] + '*rate.fits'))\n",
    "\n",
    "\n",
    "# this version assumes you have the same number of dithers for \n",
    "# the science, leak, and background observations\n",
    "if leak_version == 'dither':\n",
    "    for i,file in enumerate(scifiles_g1):\n",
    "        leak = sci_leakfiles_g1_all[i]\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw'+program_id+'-o'+obs_num_sci+'_'+group_sci[0]+'101_spec2_'+expnum+'_asn.json' \n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], [leak], None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "        \n",
    "    for i,file in enumerate(scifiles_g2):\n",
    "        leak = sci_leakfiles_g2_all[i]\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw'+program_id+'-o'+obs_num_sci+'_'+group_sci[1]+'101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], [leak], None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "\n",
    "    for i,file in enumerate(bgfiles_g1):\n",
    "        leak = bg_leakfiles_g1_all[i]\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw'+program_id+'-o'+obs_num_bkg+'_'+group_sci[0]+'101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], [leak], None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "\n",
    "    for i,file in enumerate(bgfiles_g2):\n",
    "        leak = bg_leakfiles_g2_all[i]\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw'+program_id+'-o'+obs_num_bkg+'_'+group_sci[1]+'101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], [leak], None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145839b7-19b4-49c6-86ac-3580cf6f306d",
   "metadata": {},
   "source": [
    "Below is some code to make `asn` files using all leakcals averaged for each science exposure. We are not currently maintaining this code (hence why it's commented out), but it was left in case it is helpful to anyone later.  \n",
    "\n",
    "You are welcome to minimize this code cell by clicking on the blue line that appears on the left side of the cell (when you are active in the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0dfb4-14c3-489d-b2b9-7b4ba9b28a60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keeping this alternate version for posterity\n",
    "# the below version will use average of all leakcals, and subtract that from each science exposure\n",
    "'''\n",
    "leakfiles_04_nrs1 = sorted(glob.glob(leakdir_sci + '/jw01355027001_04101_*nrs1_rate.fits'))\n",
    "leakfiles_04_nrs2 = sorted(glob.glob(leakdir_sci + '/jw01355027001_04101_*nrs2_rate.fits'))\n",
    "leakfiles_08_nrs1 = sorted(glob.glob(leakdir_sci + '/jw01355027001_08101_*nrs1_rate.fits'))\n",
    "leakfiles_08_nrs2 = sorted(glob.glob(leakdir_sci + '/jw01355027001_08101_*nrs2_rate.fits'))\n",
    "bg_leakfiles_04_nrs1 = sorted(glob.glob(leakdir_bg + '/jw01355028001_04101_*nrs1_rate.fits'))\n",
    "bg_leakfiles_04_nrs2 = sorted(glob.glob(leakdir_bg + '/jw01355028001_04101_*nrs2_rate.fits'))\n",
    "bg_leakfiles_08_nrs1 = sorted(glob.glob(leakdir_bg + '/jw01355028001_08101_*nrs1_rate.fits'))\n",
    "bg_leakfiles_08_nrs2 = sorted(glob.glob(leakdir_bg + '/jw01355028001_08101_*nrs2_rate.fits'))\n",
    "\n",
    "\n",
    "if leak_version == 'all':\n",
    "    for file in scifiles_02:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_04_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_04_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o027_02101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "\n",
    "    for file in scifiles_06:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_08_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_08_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o027_06101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "        \n",
    "    for file in bgfiles_02:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_04_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_04_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o028_02101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "\n",
    "    for file in bgfiles_06:\n",
    "        if \"nrs1\" in file:\n",
    "            leak = leakfiles_08_nrs1\n",
    "        elif \"nrs2\" in file:\n",
    "            leak = leakfiles_08_nrs2\n",
    "        else:\n",
    "            print('Check file names - no detector label found')\n",
    "            leak = None\n",
    "        expnum = file[-17:-10]\n",
    "        outfile = 'jw01355-o028_06101_spec2_'+expnum+'_asn.json'\n",
    "        outfile = os.path.join(level2outputdir, outfile)\n",
    "        writel2asn([file], leak, None, outfile, 'Level2')\n",
    "        print(outfile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20de94-3889-41f9-af25-858adabfc125",
   "metadata": {},
   "source": [
    "Now that we finished setting up the parameters for this stage, we can run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we run the pipeline with the asn files we just made! \n",
    "\n",
    "asnfiles_sci = glob.glob(level2outputdir+'jw'+program_id+'-o'+obs_num_sci+'*asn.json')\n",
    "asnfiles_bkg = glob.glob(level2outputdir+'jw'+program_id+'-o'+obs_num_bkg+'*asn.json')\n",
    "\n",
    "# run stage 2 on science frames\n",
    "for asn in asnfiles_sci:\n",
    "    spec2 = Spec2Pipeline()\n",
    "    spec2.output_dir = output_path + 'L2b/sci'\n",
    "    spec2.save_results = True\n",
    "    spec2(asn)\n",
    "\n",
    "\n",
    "# run stage 2 on background/sky frames\n",
    "for asn in asnfiles_bkg:\n",
    "    spec2 = Spec2Pipeline()\n",
    "    spec2.output_dir = output_path + 'L2b/bkg/'\n",
    "    spec2.save_results = True\n",
    "    spec2(asn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1754446",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------\n",
    "\n",
    "## Stage 3 - Creating final data cubes\n",
    "This stage compiles the individual calibrated exposures into a single, (theoretically) science-ready data cube. We first have to define an association file, which tells the pipeline which files to use as science exposures, and which files to use as backgrounds.\n",
    "\n",
    "As mentioned above, for this iteration we are doing a \"master\" background subtraction at this stage. This creates a single 1D spectrum from an average of the dedicated background exposures (using the extracted 1D spectra from the previous stage). This averaged spectrum is then universally subtracted from the science data. \n",
    "\n",
    "#### NOTE!!!!\n",
    ">#### outlier_detection is currently dysfuntional!!\n",
    ">As of pipeline version 1.8.3, the outlier_detection step of the Stage 3 pipeline is performing poorly. Conversations with the helpdesk indicate that they are aware of the issue and are looking into solutions. For now, the recommendation is to simply skip this step. \n",
    ">\n",
    ">The S1723 data are quite noisy, so some level of outlier detection is needed. We therefore include a band-aid solution, wherein we make custom cuts on the Stage 2 products (cal.fits files). These cuts need to be checked for each science target, but we find that brightness values between -1 and 20 MJy/sr work well enough for now. This interval removes the most egregious noise without sacrificing flux in the brightest emission lines, but some artifacts still appear in the final data cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW COPIED FROM DAVID LAW'S MIRI MRS NOTEBOOK: \n",
    "# https://github.com/STScI-MIRI/MRS-ExampleNB/blob/main/Flight_Notebook1/MRS_FlightNB1.ipynb\n",
    "# \n",
    "# Define a useful function to write out a Lvl3 association file from an input list\n",
    "# Note that any background exposures have to be of type x1d.\n",
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    # Add background files to the association\n",
    "    if bgfiles:\n",
    "        nbg=len(bgfiles)\n",
    "        for ii in range(0,nbg):\n",
    "            asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ffe67",
   "metadata": {},
   "source": [
    "#### *UNTIL THE OUTLIER_DETECTION IS FIXED:*\n",
    "\n",
    ">The `spec3.outlier_detection` step is currently broken (our team asked the help desk, and their response was \"yeah don't use that right now\"). So to get around this, we're going to apply some cuts to the level 2 data products to get rid of most of the obvious noise. This is far from perfect, but it will work as a band-aid solution until the actual outlier detection code is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d83853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COURTESY OF D. LAW:\n",
    "# outlier detection step is currently broken. \n",
    "# So instead, here we will manually remove outliers from the cal.fits files\n",
    "\n",
    "from stcal import dqflags\n",
    "\n",
    "\n",
    "def cut_cal(infile, outfile, max_threshold=20, min_threshold=-1):\n",
    "    hdu=fits.open(infile)\n",
    "    sci=hdu['SCI'].data\n",
    "    dq=hdu['DQ'].data\n",
    "    \n",
    "    dnubit=dqflags.interpret_bit_flags('DO_NOT_USE', mnemonic_map=datamodels.dqflags.pixel)\n",
    "    indx=np.where((dq & dnubit) != 0)\n",
    "    sci[indx]=np.nan\n",
    "\n",
    "    indx=np.where((sci > max_threshold) | (sci < min_threshold))\n",
    "    sci[indx]=np.nan\n",
    "    dq[indx] = np.bitwise_or(dq[indx], dnubit)\n",
    "    \n",
    "    hdu['SCI'].data=sci\n",
    "    hdu.writeto(outfile, overwrite=True)\n",
    "    \n",
    "# fix this path later\n",
    "orig_calfiles = glob.glob(level2outputdir + '/sci/*cal.fits') # cal files output from L2 pipeline\n",
    "\n",
    "for file in orig_calfiles:\n",
    "    outfile = file[:-5] + '2.fits'\n",
    "    cut_cal(file, outfile, max_threshold=20, min_threshold=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make association files - use sci for data, bkgd for background\n",
    "calfiles = glob.glob(output_path + 'L2b/sci/*cal2.fits')\n",
    "\n",
    "bkgfiles = glob.glob(output_path + 'L2b/bkg/*x1d.fits')\n",
    "\n",
    "asnfile = os.path.join(output_path, 'L3/L3asn.json')\n",
    "\n",
    "writel3asn(calfiles, bkgfiles, asnfile, 'Level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e968d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Next step, call stage 3 pipeline on new asn file\n",
    "asn = os.path.join(output_path, 'L3/L3asn.json')\n",
    "\n",
    "# setting it up this way (rather than with \"call\") gives a bit more flexibility to edit parameters on the fly\n",
    "spec3 = Spec3Pipeline()\n",
    "spec3.output_dir = output_path + 'L3/'\n",
    "\n",
    "# Outlier detection is broken, so we skip it for now\n",
    "spec3.outlier_detection.skip = True \n",
    "spec3.save_results = True # DON'T FORGET THIS OR YOU'LL WASTE SEVERAL HOURS FOR NAUGHT!\n",
    "\n",
    "# run stage 3\n",
    "spec3.run(asn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aabf989-6109-474b-8c6f-c836ca62578d",
   "metadata": {},
   "source": [
    "## Congrats, you're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
